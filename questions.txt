a) Time-Generalized MVPA
Similar to the paper, you could train a classifier on the random condition (least predictable) and then test it across partially and fully ordered sequences.
This allows you to measure how much the brain's predictive pattern generalizes to more predictable contexts‚Äîessentially quantifying prediction strength.
b) Regression of Prediction Strength
For each participant, you could compute classifier performance over time and then regress it against the sequence predictability (0.25, 0.75, 1.0).
This gives a slope (Œ≤) per subject, reflecting how sensitive their auditory system is to predictability.
You can then compare slopes between tinnitus and control groups, as the paper did.
c) Pre-Stimulus Activity
The paper emphasizes that predictive coding effects appear before stimulus onset.
Focus on time windows before each tone, not just the evoked response, to detect anticipatory differences in tinnitus.
d) Condition-Specific Patterns
You could explore whether certain triggers (e.g., partially ordered sequences) produce intermediate prediction responses.
This helps identify non-linear effects of predictability, since your 75% ordered condition may reveal subtle prediction deficits not visible in fully ordered or fully random sequences.

***

Decoding logic
Classifier: multiclass LDA
Train only on random (4 classes) using post-stim window
Temporal generalization: train at each time point in the post window; test:
on ordered post (post‚Üípost, cross-decoding across condition)
on ordered pre (post‚Üípre, the prediction test)
optionally on random with 5-fold CV (within-condition validation)
Balancing: equalize trial counts per class (undersample the majority if needed)
Group level: compute scores per subject, then average and do stats.

***

******

Testing on pre-stimulus
In the random condition, the next tone is unpredictable. So if you train on post-stimulus and test on pre-stimulus within random, accuracy should be at chance.
This acts as a control condition ‚Üí ensures your classifier isn‚Äôt biased and there‚Äôs no artifactual decoding in pre-stimulus when there shouldn‚Äôt be.
Cross-decoding to ordered/partially ordered
Then they apply the classifier (trained on random post-stimulus) to the pre-stimulus activity in ordered sequences.
Here, if decoding > chance, it means the brain is pre-activating the neural templates for the upcoming sound ‚Äî evidence of predictive coding.

******


***********

Normally in MVPA you:
Train a classifier at one time point,
Test it at the same time point (classic time-resolved decoding).
But brain activity is dynamic ‚Äî patterns at one moment may or may not generalize later.
Temporal Generalization (King & Dehaene, 2014):
You train the classifier at each time point,
And then test it at every other time point.
This yields a 2D matrix (train-time √ó test-time) of decoding accuracy.
üîπ How to Read It
Diagonal (train = test): tells you when information is decodable at the same time point (classic decoding).
Off-diagonal generalization: tells you whether the neural code is stable across time.
If a classifier trained at 100 ms works also at 200 ms, then the representation is sustained.
If it only works exactly at 100 ms, then the representation is transient.
So temporal generalization asks:
üëâ Are neural representations stable and reusable across time, or do they change dynamically?


***********




----------------
b) Extracting Classifier Weights
Classifier weights tell you which sensors/time points contribute most to discriminating the classes.
In the paper, they modified MVPA-Light to extract these weights, allowing them to map neural patterns associated with each tone frequency.
c) Four Target Classes
Each trial presented one of four sound frequencies, so the classifier was trained to predict which frequency was presented based on the MEG/EEG data.
This is a 4-class classification problem.
d) Training Only on Random Sequences
Important: they trained only on the random sequences (least predictable, 25%).
Why?
Random sequences avoid carryover/prediction effects from previous tones.
Ensures the classifier learns pure frequency-related neural templates, not predictive activity.
Once trained, they tested on partially and fully ordered sequences, revealing how predictions modulate neural representations.




------------------
1. Multiclass Linear Discriminant Analysis (LDA)
They trained a multiclass LDA classifier.
Multiclass LDA: a simple linear classifier that finds linear boundaries between multiple classes (here, the 4 sound frequencies).
‚ÄúOn each sample point‚Äù:
They trained separate classifiers at each time point (e.g., every millisecond or data sample in the epoch).
This is sometimes called time-resolved decoding, giving accuracy over time for each subject.

2. Averaging Accuracy Across Subjects
After decoding, they averaged the classification accuracy for each subject, then performed group-level comparisons.
This means the reported accuracy curves are mean accuracy across all subjects, giving a robust estimate.

3. Temporal Generalization (King & Dehaene, 2014)
Temporal generalization tests whether the pattern learned at one time point can predict other time points.
Example:
Train on 100‚Äì120 ms post-stimulus ‚Üí test on 150 ms ‚Üí see if the neural pattern generalizes.
This creates a time √ó time decoding matrix, showing how stable neural patterns are over time.

4. Cross-Decoding
When testing on ordered sequences:
They did not perform cross-validation because it‚Äôs cross-decoding: train on random tones, test on ordered tones.
Cross-validation is only needed when training and testing on the same condition (like random tones) to avoid overfitting.
For testing on random tones:
They did 5-fold cross-validation (splitting random tones into 5 subsets, training on 4, testing on 1, repeated 5 times).

5. Pre- vs Post-Stimulus Training
They trained on post-stimulus intervals (after the tone onset).
Tested on pre-stimulus intervals (before tone onset) for random tones.
Purpose:
This tests for predictive neural activity: can the classifier trained on responses to the sound detect pre-stimulus neural templates, reflecting anticipation or prediction.






-----------------
. Random tones ‚Üí Post-stimulus activity
In random trials, the brain cannot predict the next tone (no structure).
So, the classifier learns sensory-driven neural patterns in the 0‚Äì500 ms post-stimulus window, reflecting how the brain encodes each carrier frequency after it is heard.
This is the ground truth neural template for each tone frequency.


In ordered sequences, tones follow a predictable pattern (e.g., ascending or repeating order).
Before the next tone actually arrives, the brain can internally pre-activate the sensory representation of the expected frequency.
This predictive pre-activation should resemble the actual post-stimulus response from the random condition (because the ‚Äútemplate‚Äù is the same).

If the pre-stimulus signal in ordered trials looks similar to the post-stimulus sensory template from random trials,
‚Üí the classifier trained on random-post will generalize above chance to ordered-pre.
If there‚Äôs no predictive coding, accuracy in ordered-pre would stay at chance (~25% for 4 classes).